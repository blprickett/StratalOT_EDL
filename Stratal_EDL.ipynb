{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stratal-EDL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJG0K9wkFAB7",
        "colab_type": "text"
      },
      "source": [
        "# Stratal EDL Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdAL0-paCYaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import operator\n",
        "from random import choice\n",
        "from sys import exit\n",
        "from matplotlib.pyplot import plot, ylabel, show, xlabel, ylim, xlim, legend, title\n",
        "from scipy.stats import sem\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3rsPky-CdgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###USER SETTINGS###\n",
        "EPOCHS = 100\n",
        "SAMPLES = 50\n",
        "STRAT_NUM = 2 #This has to be 2 right now\n",
        "LEARNING_RATE = .05\n",
        "LANGUAGE = \"esi_CB\"\n",
        "WD = \"Opacity\"\n",
        "REPS = 10\n",
        "TEST_DATUM = \"ise\"\n",
        "#TEST_DATUM_OUTPUTS = {\"f\":\"esi\", \"o\":\"eSe\", \"t\":\"ese\", \"jp\":\"eSi\"} #F/CF   {faithful, opaque, transparent, just palatalization}\n",
        "TEST_DATUM_OUTPUTS = {\"f\":\"ise\", \"o\":\"isi\", \"t\":\"iSi\", \"jp\":\"iSe\"} #B/CB    {faithful, opaque, transparent, just palatalization}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu4EupOICxRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount the user's google drive:\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QMAvsLE7vN",
        "colab_type": "text"
      },
      "source": [
        "## Process Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sIP0JCaCgMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###PROCESS INPUT###\n",
        "#CON:\n",
        "CON_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Input Files/\"+LANGUAGE+\"_CON.txt\", \"r\")\n",
        "marked_con = {}\n",
        "faithful_con = {}\n",
        "mark = False\n",
        "for line in CON_file.readlines():\n",
        "    if \"markedness\" in line:\n",
        "        mark = True\n",
        "        continue\n",
        "    elif \"faithfulness\" in line:\n",
        "        mark = False\n",
        "        continue\n",
        "        \n",
        "    if mark:\n",
        "        name, regex = line.rstrip().split(\"\\t\")\n",
        "        marked_con[name] = regex\n",
        "    else:\n",
        "        name, this_ur, this_sr = line.rstrip().split(\"\\t\")\n",
        "        faithful_con[name] = [this_ur.split(\",\"), this_sr.split(\",\")]\n",
        "CON = list(marked_con.keys()) + list(faithful_con.keys())\n",
        "CON_file.close()\n",
        "\n",
        "#GEN:\n",
        "GEN_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Input Files/\"+LANGUAGE+\"_GEN.txt\", \"r\")\n",
        "ur2cands = {}\n",
        "for line in GEN_file.readlines():\n",
        "    this_in, these_outs = line.rstrip().split(\"\\t\")\n",
        "    cands = these_outs.split(\",\")\n",
        "    ur2cands[this_in] = cands\n",
        "GEN_file.close()\n",
        "\n",
        "#Training Data\n",
        "TD_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Input Files/\"+LANGUAGE+\"_Dist.txt\", \"r\")\n",
        "TD_input = []\n",
        "TD_output = []\n",
        "for line in TD_file.readlines():\n",
        "    data_line = re.search('\"x\"\\t\"(.+)\"\\t\\t([0-9]+)\\t[0-9]+\\t(.+)', line)\n",
        "    if data_line:\n",
        "        this_sr = data_line.group(1)\n",
        "        this_freq = int(data_line.group(2))\n",
        "        this_ur = data_line.group(3)\n",
        "        \n",
        "        TD_input += [this_ur] * this_freq\n",
        "        TD_output += [this_sr] * this_freq           \n",
        "TD_file.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAJV-PjtFEzc",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5bHe4gAD627",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Violation profiles:\n",
        "get_viol = {}\n",
        "for this_ur in ur2cands.keys():\n",
        "    for cand in ur2cands[this_ur]:\n",
        "        #Markedness\n",
        "        surface_cand = re.sub(\"_\", \"\", cand)\n",
        "        for con in marked_con.keys():\n",
        "            try:\n",
        "                get_viol[(this_ur,cand)][con] = len(re.findall(marked_con[con], surface_cand))\n",
        "            except:\n",
        "                get_viol[(this_ur,cand)] = {}\n",
        "                get_viol[(this_ur,cand)][con] = len(re.findall(marked_con[con], surface_cand))\n",
        "        \n",
        "        #Faithfulness\n",
        "        for con in faithful_con.keys():\n",
        "            get_viol[(this_ur,cand)][con] = 0\n",
        "            for faith_index, faith_input_seg in enumerate(faithful_con[con][0]):\n",
        "                faith_output_seg = faithful_con[con][1][faith_index]\n",
        "                for form_index in range(len(this_ur)):\n",
        "                    if (this_ur[form_index] == faith_input_seg) and (cand[form_index]==faith_output_seg):\n",
        "                        get_viol[(this_ur,cand)][con] += 1\n",
        "                        \n",
        "#Rankings we're keeping track of:\n",
        "    rankings = []\n",
        "    for c1 in CON:\n",
        "        for c2 in CON:\n",
        "            if c1 == c2:\n",
        "                continue\n",
        "            rankings.append(\">>\".join(sorted([c1, c2])))\n",
        "    rankings = list(set(rankings)) #Get rid of duplicate rankings\n",
        "    strat_size = len(rankings) #num of cons in each stratum\n",
        "    rankings *= STRAT_NUM #Copy the constraint set for multiple strata\n",
        "\n",
        "###CUSTOM FUNCTIONS###\n",
        "#Function for finding optimal candidate:\n",
        "def getWinner (inp, cons, cands):\n",
        "    new_cands = cands[:]\n",
        "    for con in cons:\n",
        "        these_viols = []\n",
        "        violNum2cands = {}\n",
        "        for cand in cands:\n",
        "            this_viol = get_viol[(inp, cand)][con]\n",
        "            these_viols.append(this_viol)\n",
        "            try:\n",
        "                violNum2cands[this_viol].append(cand)\n",
        "            except:\n",
        "                violNum2cands[this_viol] = [cand]\n",
        "        new_cands = violNum2cands[min(these_viols)]\n",
        "        if len(new_cands) == 1:\n",
        "            return new_cands[0]\n",
        "        else:\n",
        "            cands = new_cands[:]\n",
        "    return choice(cands)\n",
        "\n",
        "#Function that converts stratum's rankings to ordered constraints:\n",
        "def orderConstraints (stratum_names, stratum_ranks):\n",
        "    dom_counts = {c:0 for c in CON}\n",
        "    for rank_j, rank_name in enumerate(stratum_names):\n",
        "        if stratum_ranks[rank_j]:\n",
        "            dom_counts[rank_name.split(\">>\")[0]] += 1\n",
        "        else:\n",
        "            dom_counts[rank_name.split(\">>\")[1]] += 1  \n",
        "                    \n",
        "    ranked_cons = []\n",
        "    for constraint in sorted(dom_counts.items(), key=operator.itemgetter(1), reverse=True):\n",
        "        ranked_cons.append(constraint[0])\n",
        "    \n",
        "    return ranked_cons\n",
        "\n",
        "#Function for getting accuracy of a given grammar:\n",
        "def getAccuracy (grammar):\n",
        "    samp_accs = []\n",
        "    for s in range(SAMPLES):\n",
        "        sampled_grammar = np.random.rand((len(grammar))) < grammar\n",
        "        strat1_cons = orderConstraints(rankings[:strat_size], sampled_grammar[:strat_size])\n",
        "        strat2_cons = orderConstraints(rankings[strat_size:], sampled_grammar[strat_size:])\n",
        "        correct_count = 0\n",
        "        total_count = 0\n",
        "        for i, input_i in enumerate(TD_input):\n",
        "            intermed_form = getWinner(input_i, strat1_cons, ur2cands[input_i])\n",
        "            surface_form = getWinner(intermed_form, strat2_cons, ur2cands[intermed_form])\n",
        "            total_count += 1\n",
        "            if re.sub(\"_\", \"\", surface_form) == TD_output[i]:\n",
        "                correct_count += 1\n",
        "        samp_accs.append(float(correct_count)/float(total_count))\n",
        "    return np.mean(samp_accs)\n",
        "\n",
        "def accByDatum (grammar):\n",
        "    samp_accs = {d:[] for d in TD_input}\n",
        "    for s in range(SAMPLES):\n",
        "        sampled_grammar = np.random.rand((len(grammar))) < grammar\n",
        "        strat1_cons = orderConstraints(rankings[:strat_size], sampled_grammar[:strat_size])\n",
        "        strat2_cons = orderConstraints(rankings[strat_size:], sampled_grammar[strat_size:])\n",
        "        for i, input_i in enumerate(TD_input):\n",
        "            intermed_form = getWinner(input_i, strat1_cons, ur2cands[input_i])\n",
        "            surface_form = getWinner(intermed_form, strat2_cons, ur2cands[intermed_form])\n",
        "            if re.sub(\"_\", \"\", surface_form) == TD_output[i]:\n",
        "                samp_accs[input_i].append(1.0)\n",
        "            else:\n",
        "                samp_accs[input_i].append(0.0)\n",
        "    av_accs = [np.mean(samp_accs[ur]) for ur in TD_input]\n",
        "    return av_accs   \n",
        "    \n",
        "def forcedChoiceByDatum (grammar):\n",
        "    lang2ur2choice =  { \n",
        "                    'esi_B':{\n",
        "                                \"Ese\":     [\"Ese\", \"ESe\"],\n",
        "                                \"esE\":     [\"esE\", \"eSE\"],\n",
        "                                \"ake\":     [\"ake\", \"aki\"],\n",
        "                                \"akE\":     [\"akE\", \"akI\"],\n",
        "                                \"aki\":     [\"aki\", \"ake\"],\n",
        "                                \"akI\":     [\"akI\", \"akE\"],\n",
        "                                \"ase\":     [\"ase\", \"aSe\"],\n",
        "                                \"asE\":     [\"asE\", \"aSE\"],\n",
        "                                \"ekI\":     [\"ekE\", \"ekI\"],\n",
        "                                \"Eki\":     [\"Eke\", \"Eki\"],\n",
        "                                \"ikE\":     [\"ikI\", \"ikE\"],\n",
        "                                \"Ike\":     [\"Iki\", \"Ike\"],\n",
        "                                \"isI\":     [\"iSI\", \"isI\"],\n",
        "                                \"Isi\":     [\"ISi\", \"Isi\"],\n",
        "                                \"asi\":     [\"aSi\", \"asi\"],\n",
        "                                \"asI\":     [\"aSI\", \"asI\"],\n",
        "                                \"esi\":     [\"ese\", \"eSe\"],\n",
        "                                \"Esi\":     [\"Ese\", \"ESe\"],\n",
        "                                \"esI\":     [\"esE\", \"eSE\"],\n",
        "                                \"EsI\":     [\"EsE\", \"ESE\"],\n",
        "                            },\n",
        "                    'ise_F':{\n",
        "                                \"Ese\":     [\"Ese\", \"ESe\"],\n",
        "                                \"esE\":     [\"esE\", \"eSE\"],\n",
        "                                \"ake\":     [\"ake\", \"aki\"],\n",
        "                                \"akE\":     [\"akE\", \"akI\"],\n",
        "                                \"aki\":     [\"aki\", \"ake\"],\n",
        "                                \"akI\":     [\"akI\", \"akE\"],\n",
        "                                \"ase\":     [\"ase\", \"aSe\"],\n",
        "                                \"asE\":     [\"asE\", \"aSE\"],\n",
        "                                \"ekI\":     [\"ekE\", \"ekI\"],\n",
        "                                \"Eki\":     [\"Eke\", \"Eki\"],\n",
        "                                \"ikE\":     [\"ikI\", \"ikE\"],\n",
        "                                \"Ike\":     [\"Iki\", \"Ike\"],\n",
        "                                \"isI\":     [\"iSI\", \"isI\"],\n",
        "                                \"Isi\":     [\"ISi\", \"Isi\"],\n",
        "                                \"asi\":     [\"aSi\", \"asi\"],\n",
        "                                \"asI\":     [\"aSI\", \"asI\"],\n",
        "                                \"ise\":     [\"iSi\", \"isi\"],\n",
        "                                \"Ise\":     [\"ISi\", \"Isi\"],\n",
        "                                \"isE\":     [\"iSI\", \"isI\"],\n",
        "                                \"IsE\":     [\"ISI\", \"IsI\"],\n",
        "                            },\n",
        "                    'esi_CB':{\n",
        "                                \"Ese\":     [\"Ese\", \"ESe\"],\n",
        "                                \"esE\":     [\"esE\", \"eSE\"],\n",
        "                                \"ake\":     [\"ake\", \"aki\"],\n",
        "                                \"akE\":     [\"akE\", \"akI\"],\n",
        "                                \"aki\":     [\"aki\", \"ake\"],\n",
        "                                \"akI\":     [\"akI\", \"akE\"],\n",
        "                                \"ase\":     [\"ase\", \"aSe\"],\n",
        "                                \"asE\":     [\"asE\", \"aSE\"],\n",
        "                                \"ekI\":     [\"ekE\", \"ekI\"],\n",
        "                                \"Eki\":     [\"Eke\", \"Eki\"],\n",
        "                                \"ikE\":     [\"ikI\", \"ikE\"],\n",
        "                                \"Ike\":     [\"Iki\", \"Ike\"],\n",
        "                                \"isI\":     [\"iSI\", \"isI\"],\n",
        "                                \"Isi\":     [\"ISi\", \"Isi\"],\n",
        "                                \"asi\":     [\"aSi\", \"asi\"],\n",
        "                                \"asI\":     [\"aSI\", \"asI\"],\n",
        "                                \"esi\":     [\"eSe\", \"ese\"],\n",
        "                                \"Esi\":     [\"ESe\", \"Ese\"],\n",
        "                                \"esI\":     [\"eSE\", \"esE\"],\n",
        "                                \"EsI\":     [\"ESE\", \"EsE\"],\n",
        "                            },\n",
        "                    'ise_CF':{\n",
        "                                \"Ese\":     [\"Ese\", \"ESe\"],\n",
        "                                \"esE\":     [\"esE\", \"eSE\"],\n",
        "                                \"ake\":     [\"ake\", \"aki\"],\n",
        "                                \"akE\":     [\"akE\", \"akI\"],\n",
        "                                \"aki\":     [\"aki\", \"ake\"],\n",
        "                                \"akI\":     [\"akI\", \"akE\"],\n",
        "                                \"ase\":     [\"ase\", \"aSe\"],\n",
        "                                \"asE\":     [\"asE\", \"aSE\"],\n",
        "                                \"ekI\":     [\"ekE\", \"ekI\"],\n",
        "                                \"Eki\":     [\"Eke\", \"Eki\"],\n",
        "                                \"ikE\":     [\"ikI\", \"ikE\"],\n",
        "                                \"Ike\":     [\"Iki\", \"Ike\"],\n",
        "                                \"isI\":     [\"iSI\", \"isI\"],\n",
        "                                \"Isi\":     [\"ISi\", \"Isi\"],\n",
        "                                \"asi\":     [\"aSi\", \"asi\"],\n",
        "                                \"asI\":     [\"aSI\", \"asI\"],\n",
        "                                \"ise\":     [\"isi\", \"iSi\"],\n",
        "                                \"Ise\":     [\"Isi\", \"ISi\"],\n",
        "                                \"isE\":     [\"isI\", \"iSI\"],\n",
        "                                \"IsE\":     [\"IsI\", \"ISI\"],\n",
        "                            }\n",
        "                  }\n",
        "    samp_accs = {d:[] for d in TD_input}\n",
        "    for s in range(SAMPLES):\n",
        "        sampled_grammar = np.random.rand((len(grammar))) < grammar\n",
        "        strat1_cons = orderConstraints(rankings[:strat_size], sampled_grammar[:strat_size])\n",
        "        strat2_cons = orderConstraints(rankings[strat_size:], sampled_grammar[strat_size:])\n",
        "        for i, input_i in enumerate(TD_input):\n",
        "            intermed_form = getWinner(input_i, strat1_cons, ur2cands[input_i])\n",
        "            surface_form = getWinner(intermed_form, strat2_cons, ur2cands[intermed_form])\n",
        "            if re.sub(\"_\", \"\", surface_form) == lang2ur2choice[LANGUAGE][input_i][0]:\n",
        "                samp_accs[input_i].append(1.0) #Correct choice\n",
        "            elif re.sub(\"_\", \"\", surface_form) == lang2ur2choice[LANGUAGE][input_i][1]:\n",
        "                samp_accs[input_i].append(0.0) #Incorrect choice\n",
        "    av_accs = [np.mean(samp_accs[ur]) for ur in TD_input]\n",
        "    return av_accs   \n",
        "    \n",
        "def sampleTestDatum (grammar):\n",
        "    sample_counts = {}\n",
        "    for s in range(SAMPLES):\n",
        "        sampled_grammar = np.random.rand((len(grammar))) < grammar\n",
        "        strat1_cons = orderConstraints(rankings[:strat_size], sampled_grammar[:strat_size])\n",
        "        strat2_cons = orderConstraints(rankings[strat_size:], sampled_grammar[strat_size:])\n",
        "        intermed_form = getWinner(TEST_DATUM, strat1_cons, ur2cands[TEST_DATUM])\n",
        "        surface_form = getWinner(intermed_form, strat2_cons, ur2cands[intermed_form])\n",
        "        try:\n",
        "            sample_counts[surface_form] += 1.0\n",
        "        except:\n",
        "            sample_counts[surface_form] = 1.0\n",
        "    \n",
        "    return sample_counts  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhUhc64eFHRR",
        "colab_type": "text"
      },
      "source": [
        "## Run Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COAuiTt_ExOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###START SIMULATIONS###\n",
        "solution_counts = {}\n",
        "learning_curves = []\n",
        "datum_curves = {ur:[] for ur in TD_input}\n",
        "datum_fc_curves = {ur:[] for ur in TD_input}\n",
        "test_probs = {} \n",
        "gen_curves = {v:[[] for r in range(REPS)] for v in TEST_DATUM_OUTPUTS.values()}  \n",
        "all_rank_probs = {r+\" (Stratum:\"+str(st+1)+\")\":[] for r in rankings for st in range(STRAT_NUM)}\n",
        "for rep in range(REPS):                \n",
        "    #Initialize the grammar:\n",
        "    rank_probs = np.array([.5 for rank in rankings])\n",
        "    \n",
        "    #Start learning:\n",
        "    con_ranks_s1 = {c:[] for c in CON}\n",
        "    con_ranks_s2 = {c:[] for c in CON}\n",
        "    this_curve = []\n",
        "    this_datum_curve = {ur:[] for ur in TD_input}\n",
        "    this_datum_fc_curve = {ur:[] for ur in TD_input}\n",
        "    for epoch in range(EPOCHS):\n",
        "        if epoch % 25 == 0:\n",
        "          print (\"Epoch: \", epoch, \"Rep: \", rep)\n",
        "        av_ranks1 = {}\n",
        "        av_ranks2 = {}\n",
        "        for TD_index, this_UR in enumerate(TD_input):\n",
        "            new_grammar = [0 for r in rankings]\n",
        "            for rank_i, ranking in enumerate(rankings):\n",
        "                freq_given_true = 0\n",
        "                freq_given_false = 0\n",
        "                for sample in range(SAMPLES):\n",
        "                    #Sample grammar:\n",
        "                    sampled_grammar = np.random.rand((len(rankings))) < rank_probs\n",
        "                    \n",
        "                    #Set the ranking of interest to true/false:\n",
        "                    true_grammar = np.copy(sampled_grammar)\n",
        "                    true_grammar[rank_i] = True\n",
        "                    false_grammar = np.copy(sampled_grammar)\n",
        "                    false_grammar[rank_i] = False              \n",
        "                    \n",
        "                    #Get the ordered list of constraints:\n",
        "                    true_dom_counts_s1 = {c:0 for c in CON}\n",
        "                    false_dom_counts_s1 = {c:0 for c in CON}\n",
        "                    true_dom_counts_s2 = {c:0 for c in CON}\n",
        "                    false_dom_counts_s2 = {c:0 for c in CON}\n",
        "                    for rank_j, rank_name in enumerate(rankings):\n",
        "                        if rank_j < strat_size:\n",
        "                        #Stratum 1:\n",
        "                            #True:\n",
        "                            if true_grammar[rank_j]:\n",
        "                                true_dom_counts_s1[rank_name.split(\">>\")[0]] += 1\n",
        "                            else:\n",
        "                                true_dom_counts_s1[rank_name.split(\">>\")[1]] += 1\n",
        "                            #False:\n",
        "                            if false_grammar[rank_j]:\n",
        "                                false_dom_counts_s1[rank_name.split(\">>\")[0]] += 1\n",
        "                            else:\n",
        "                                false_dom_counts_s1[rank_name.split(\">>\")[1]] += 1\n",
        "                        else:\n",
        "                        #Stratum 2:\n",
        "                            #True:\n",
        "                            if true_grammar[rank_j]:\n",
        "                                true_dom_counts_s2[rank_name.split(\">>\")[0]] += 1\n",
        "                            else:\n",
        "                                true_dom_counts_s2[rank_name.split(\">>\")[1]] += 1\n",
        "                            #False:\n",
        "                            if false_grammar[rank_j]:\n",
        "                                false_dom_counts_s2[rank_name.split(\">>\")[0]] += 1\n",
        "                            else:\n",
        "                                false_dom_counts_s2[rank_name.split(\">>\")[1]] += 1    \n",
        "                                \n",
        "                    #Stratum 1\n",
        "                    true_ranked_s1 = []\n",
        "                    false_ranked_s1 = []\n",
        "                    for constraint in sorted(true_dom_counts_s1.items(), key=operator.itemgetter(1), reverse=True):\n",
        "                        true_ranked_s1.append(constraint[0])\n",
        "                        #con_ranks_s1[constraint[0]].append(true_dom_counts_s1[constraint[0]])\n",
        "                    for constraint in sorted(false_dom_counts_s1.items(), key=operator.itemgetter(1), reverse=True):\n",
        "                        false_ranked_s1.append(constraint[0])\n",
        "                        #con_ranks_s1[constraint[0]].append(false_dom_counts_s1[constraint[0]])\n",
        "                    #Stratum 2\n",
        "                    true_ranked_s2 = []\n",
        "                    false_ranked_s2 = []\n",
        "                    for constraint in sorted(true_dom_counts_s2.items(), key=operator.itemgetter(1), reverse=True):\n",
        "                        true_ranked_s2.append(constraint[0])\n",
        "                        #con_ranks_s2[constraint[0]].append(true_dom_counts_s2[constraint[0]])\n",
        "                    for constraint in sorted(false_dom_counts_s2.items(), key=operator.itemgetter(1), reverse=True):\n",
        "                        false_ranked_s2.append(constraint[0])\n",
        "                        #con_ranks_s2[constraint[0]].append(false_dom_counts_s2[constraint[0]])\n",
        "                        \n",
        "                    #Get predicted winners, check them:\n",
        "                    #Stratum 1\n",
        "                    true_s1_winner = getWinner(this_UR, true_ranked_s1, ur2cands[this_UR])\n",
        "                    false_s1_winner = getWinner(this_UR, false_ranked_s1, ur2cands[this_UR])\n",
        "                    #Stratum 2\n",
        "                    true_s2_winner = getWinner(true_s1_winner, true_ranked_s2, ur2cands[true_s1_winner])\n",
        "                    false_s2_winner = getWinner(false_s1_winner, false_ranked_s2, ur2cands[false_s1_winner])\n",
        "                    \n",
        "                    #Record learning curve info:\n",
        "                    for c in CON:\n",
        "                        try:\n",
        "                            av_ranks1[c].append(true_ranked_s1.index(c))\n",
        "                            av_ranks2[c].append(true_ranked_s2.index(c))\n",
        "                        except:\n",
        "                            av_ranks1[c] = [true_ranked_s1.index(c)]\n",
        "                            av_ranks2[c] = [true_ranked_s2.index(c)]\n",
        "                    \n",
        "                    if re.sub(\"_\", \"\", true_s2_winner) == TD_output[TD_index]:\n",
        "                        freq_given_true += 1.0\n",
        "                    if re.sub(\"_\", \"\", false_s2_winner) == TD_output[TD_index]:\n",
        "                        freq_given_false += 1.0\n",
        "                \n",
        "                #Use the sample drawn above to calculate the necessary probs:    \n",
        "                prob_given_true = (freq_given_true/SAMPLES) + .0001\n",
        "                prob_given_false = (freq_given_false/SAMPLES) + .0001\n",
        "                probDatumGivenGrammar = (prob_given_true * rank_probs[rank_i]) + \\\n",
        "                                        (prob_given_false * (1.0 - rank_probs[rank_i]))\\\n",
        "                                        + .0001\n",
        "                new_grammar[rank_i] = (prob_given_true * rank_probs[rank_i])/probDatumGivenGrammar\n",
        "            \n",
        "            rank_probs = (np.array(new_grammar)*LEARNING_RATE) + (rank_probs*(1-LEARNING_RATE))\n",
        "            \n",
        "        #Store learning curve information:\n",
        "        for c in CON:\n",
        "            con_ranks_s1[c].append(-1*np.mean(av_ranks1[c]))\n",
        "            con_ranks_s2[c].append(-1*np.mean(av_ranks2[c]))\n",
        "        this_curve.append(getAccuracy(rank_probs))\n",
        "        these_accs = accByDatum(rank_probs)\n",
        "        these_fc_accs = forcedChoiceByDatum(rank_probs)\n",
        "        for i, input_i in enumerate(TD_input):\n",
        "            this_datum_curve[input_i].append(these_accs[i])\n",
        "            this_datum_fc_curve[input_i].append(these_fc_accs[i])\n",
        "\n",
        "        #Store generalization curve information:\n",
        "        testCounts = sampleTestDatum(rank_probs)\n",
        "        totalCount = float(sum(testCounts.values()))\n",
        "        for test_type in TEST_DATUM_OUTPUTS.keys():\n",
        "          sr = TEST_DATUM_OUTPUTS[test_type]\n",
        "          if sr in testCounts.keys():\n",
        "            try:\n",
        "              gen_curves[sr][rep].append(testCounts[sr]/totalCount)\n",
        "            except:\n",
        "              print (\"\\n\\n!!\\n\", sr, rep, \"\\n\\n!!\\n\")\n",
        "              raise Exception\n",
        "          else:\n",
        "            try:\n",
        "              gen_curves[sr][rep].append(0.0)\n",
        "            except:\n",
        "              print (\"\\n\\n!!\\n\", gen_curves.keys())\n",
        "              print (\"\\n\\n!!\\n\", sr, rep, \"\\n\\n!!\\n\")\n",
        "              raise Exception\n",
        "\n",
        "    #Track solutions that were found:\n",
        "    final_sample = np.random.rand((len(rankings))) < rank_probs\n",
        "    final_strat1 = orderConstraints(rankings[:strat_size], final_sample[:strat_size])\n",
        "    final_strat2 = orderConstraints(rankings[strat_size:], final_sample[strat_size:])\n",
        "    solution = \"Strat1: \"+\">>\".join(final_strat1)+\", Strat2: \"+\">>\".join(final_strat2) \n",
        "    try:\n",
        "        solution_counts[solution][0] += 1\n",
        "        solution_counts[solution][1].append(this_curve[-1])\n",
        "    except:\n",
        "        solution_counts[solution] = [1, [this_curve[-1]]] \n",
        "        \n",
        "    #Track ranking probs that were found:\n",
        "    ranks_so_far = []\n",
        "    strat = 1\n",
        "    for rank_i, rank_name in enumerate(rankings):\n",
        "        if rank_name in ranks_so_far:\n",
        "            strat = 2\n",
        "        ranks_so_far.append(rank_name)\n",
        "        all_rank_probs[rank_name+\" (Stratum:\"+str(strat)+\")\"].append(rank_probs[rank_i])   \n",
        "        \n",
        "    #Track learning:\n",
        "    learning_curves.append(this_curve)\n",
        "    for i, input_i in enumerate(TD_input):\n",
        "            datum_curves[input_i].append(this_datum_curve[input_i])\n",
        "            datum_fc_curves[input_i].append(this_datum_fc_curve[input_i])\n",
        "    \n",
        "    #Track generalization to withheld data:\n",
        "    testCounts = sampleTestDatum(rank_probs)\n",
        "    totalCount = float(sum(testCounts.values()))\n",
        "    for sampled_output in testCounts.keys():\n",
        "        try:\n",
        "            test_probs[sampled_output].append(testCounts[sampled_output]/totalCount)\n",
        "        except:\n",
        "            test_probs[sampled_output] = [testCounts[sampled_output]/totalCount]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiEO9zoaFJZw",
        "colab_type": "text"
      },
      "source": [
        "## Display/Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-1gou7PE0-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###SAVE/PLOT RESULTS###\n",
        "#Save test data output:\n",
        "testData_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_testData (reps=\"+str(REPS)+\").csv\", \"w\")\n",
        "for output in test_probs.keys():\n",
        "    full_list = test_probs[output]+[0.0]*(REPS-len(test_probs[output])) \n",
        "    testData_file.write(output+\",\"+str(np.mean(full_list))+\",\"+str(sem(full_list))+\"\\n\")  \n",
        "testData_file.close()         \n",
        "\n",
        "#Save overall learning curve:\n",
        "curve_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_curveOutput (reps=\"+str(REPS)+\").txt\", \"w\")\n",
        "av_curve = np.mean(learning_curves, axis=0)\n",
        "for epoch in av_curve:\n",
        "    curve_file.write(str(epoch)+\"\\n\")\n",
        "curve_file.close()\n",
        "\n",
        "#Save solution counts:\n",
        "solution_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_solutionCountOutput (reps=\"+str(REPS)+\").txt\", \"w\")\n",
        "for sol in solution_counts.keys():\n",
        "    solution_file.write(sol+\"\\t\"+str(solution_counts[sol][0])+\"\\t\"+str(np.mean(solution_counts[sol][1]))+\"\\n\")\n",
        "solution_file.close()\n",
        "\n",
        "#Save ranking prob distributions:\n",
        "rankProb_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_rankingProbs (reps=\"+str(REPS)+\").csv\", \"w\")\n",
        "for strat in range(1,STRAT_NUM+1):\n",
        "    rankProb_file.write(\"STRATUM: \"+str(strat)+\"\\n\")\n",
        "    rankProb_file.write(\",\"+\",\".join(CON)+\"\\n\")\n",
        "    for row in CON:\n",
        "        rankProb_file.write(row+\",\")\n",
        "        for column in CON:\n",
        "            if column == row:\n",
        "                rankProb_file.write(\"-,\")\n",
        "                continue\n",
        "            try:\n",
        "                these_probs = all_rank_probs[row+\">>\"+column+\" (Stratum:\"+str(strat)+\")\"]\n",
        "            except:\n",
        "                these_probs = [1-rp for rp in all_rank_probs[column+\">>\"+row+\" (Stratum:\"+str(strat)+\")\"]]\n",
        "            average = np.around(np.mean(these_probs), 3)\n",
        "            sd = np.around(np.std(these_probs), 3)\n",
        "            rankProb_file.write(str(average)+\" (\"+str(sd)+\"),\")\n",
        "        rankProb_file.write(\"\\n\")               \n",
        "rankProb_file.close()\n",
        "\n",
        "#Save generalization curves:\n",
        "genCurve_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_GenCurves (reps=\"+str(REPS)+\").csv\", \"w\")\n",
        "for sr in gen_curves.keys():\n",
        "    for rep_ix, rep in enumerate(gen_curves[sr]): \n",
        "      genCurve_file.write(sr+\",\"+\"Rep:\"+str(rep_ix)+\",\"+\",\".join([str(p) for p in rep])+\"\\n\")  \n",
        "genCurve_file.close() \n",
        "\n",
        "\n",
        "#Plot datum (and forced choice) accuracy curves: \n",
        "byDatum_file = open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_byDatumOutput (reps=\"+str(REPS)+\").csv\", \"w\")   \n",
        "forcedChoice_file =   open(\"/content/gdrive/My Drive/\"+WD+\"/Output Files/\"+LANGUAGE+\"_forcedChoiceOutput (reps=\"+str(REPS)+\").csv\", \"w\")             \n",
        "for datum in datum_curves.keys():\n",
        "    av_datum_curve = np.mean(datum_curves[datum], axis=0)\n",
        "    #av_fc_curve = np.mean(datum_fc_curves[datum], axis=0)\n",
        "    byDatum_file.write(datum+\",\"+\",\".join([str(a) for a in av_datum_curve])+\"\\n\")\n",
        "    #forcedChoice_file.write(datum+\",\"+\",\".join([str(a) for a in av_fc_curve])+\"\\n\")\n",
        "\n",
        "    for rep, curve in enumerate(datum_fc_curves[datum]):\n",
        "      this_row = [str(rep), str(datum)]\n",
        "      for epoch, prob in enumerate(curve):\n",
        "        this_row.append(str(prob))\n",
        "      forcedChoice_file.write(\",\".join(this_row)+\"\\n\")\n",
        "\n",
        "\n",
        "    plot(av_datum_curve, label=datum, linestyle=\"-\")\n",
        "byDatum_file.close()\n",
        "forcedChoice_file.close()\n",
        "\n",
        "l = legend()\n",
        "xlabel(\"Epoch\")\n",
        "ylabel(\"Average Percent Correct\")\n",
        "title(LANGUAGE)\n",
        "show() "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}